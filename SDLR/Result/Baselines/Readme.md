The proposed method is evaluated on usual and noisy data for comparing the performance and robustness of it which are reported in `Not_Noise` and `Noisy` directories.

We compare our learning to rank framework with a number of prominent and state-of-the-art baselines namely [ListNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf), [ListMLE](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwinnOP6tIqEAxX7gf0HHbxvB7kQFnoECBcQAQ&url=http%3A%2F%2Ficml2008.cs.helsinki.fi%2Fpapers%2F167.pdf&usg=AOvVaw06tFNflKZXBbTGWegMw5wz&opi=89978449), [ApproxNDCG](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2008-164.pdf) , [XEùëõùëëùëêùëîMART](https://arxiv.org/abs/1911.09798), [SetRank](https://arxiv.org/abs/1912.05891), [OrdinalRank](https://arxiv.org/abs/2005.10084), and [NeuralNDCG](https://arxiv.org/abs/2102.07831). We also included baselines that adopt knowledge distillation approach in learning to rank models, namely [RankDistil](https://proceedings.mlr.press/v130/reddi21a.html) and [SDR](https://proceedings.mlr.press/v130/reddi21a.html), and baselines that target training generalizable models: [ListMAP](https://www.sciencedirect.com/science/article/abs/pii/S0306457322000802), [SOUR](https://dl.acm.org/doi/pdf/10.1145/3539813.3545127) , and Sigmoid-Loss.
