The results include of three various implementation for performance examination of SDLR in different setting and for showing the performance of Knowledge Distillation model which contains of teacher and student phase:

1. Data were split with different ratio between teacher and student in `Different ratio of Data`.
2. Give all data just to the teacher phase (without having student phase) to show the performance of Knowledge Distillation model in `Teacher Only`.
3. Train both teacher and student on all the data for examination of tranfered knowledge between teacher and student phase`Train over 100% of training data`.
